{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5aV5mzQI804"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv1d(1,16,5)\r\n",
        "    self.conv2 = nn.Conv1d(16,32,5)\r\n",
        "    self.conv3 = nn.Conv1d(32,64,5)\r\n",
        "\r\n",
        "    self.fc1 = nn.Linear(159744,2)\r\n",
        "\r\n",
        "    self.norm1 = nn.BatchNorm1d(1)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "\r\n",
        "    x = self.norm1(x)\r\n",
        "    x = F.relu(self.conv1(x))\r\n",
        "    x = F.max_pool1d(x,2)\r\n",
        "    x = F.relu(self.conv2(x))\r\n",
        "    x = F.max_pool1d(x,2)\r\n",
        "    x = F.relu(self.conv3(x))\r\n",
        "    x = F.max_pool1d(x,2)\r\n",
        "    x = torch.flatten(x,start_dim=1)\r\n",
        "    x = F.log_softmax(self.fc1(x),dim=1)\r\n",
        "    \r\n",
        "    return x\r\n",
        "\r\n",
        "net = Net()\r\n",
        "net = net.double()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "Kbz6sxs7I83u",
        "outputId": "a00a12c2-4225-40c0-faea-669ed103bd33"
      },
      "source": [
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch.optim as optim\r\n",
        "import torch.tensor as tensor\r\n",
        "import torch\r\n",
        "\r\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.001)\r\n",
        "\r\n",
        "filenames = [line[2] for line in os.walk('/content/drive/MyDrive/fft')][0]\r\n",
        "\r\n",
        "batch_size = 30\r\n",
        "batch_number = len(filenames)/batch_size\r\n",
        "epochs = 1\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "\r\n",
        "  random.shuffle(filenames)\r\n",
        "  batches = [filenames[i:i+batch_size] for i in range(0,len(filenames),batch_size)]\r\n",
        "  count = 0\r\n",
        "\r\n",
        "  for batch in batches:\r\n",
        "\r\n",
        "    count += 1\r\n",
        "    labels = tensor([int('preictal' in i) for i in batch])\r\n",
        "    n1 = torch.count_nonzero(labels)\r\n",
        "    n0 = batch_size-n1\r\n",
        "    x = tensor([np.load('/content/drive/MyDrive/fft/' + file)[:20000] for file in batch])\r\n",
        "    x = x.unsqueeze(1)\r\n",
        "    \r\n",
        "    net.zero_grad()\r\n",
        "    output = net(x)\r\n",
        "    # print(output)\r\n",
        "    loss = F.nll_loss(output,labels,weight=tensor([batch_size/(2*n0),batch_size/(2*n1)]).double())\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    print('Epoch number %d/%d' % (epoch+1,epochs))\r\n",
        "    print('Batch number %d/%d' % (count,batch_number))\r\n",
        "    print('Loss is %f' % (float(loss)))\r\n",
        "    print('\\n')\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number 1/1\n",
            "Batch number 1/82\n",
            "Loss is 0.535302\n",
            "Epoch number 1/1\n",
            "Batch number 2/82\n",
            "Loss is 0.608122\n",
            "Epoch number 1/1\n",
            "Batch number 3/82\n",
            "Loss is 0.573932\n",
            "Epoch number 1/1\n",
            "Batch number 4/82\n",
            "Loss is 0.847997\n",
            "Epoch number 1/1\n",
            "Batch number 5/82\n",
            "Loss is 0.516464\n",
            "Epoch number 1/1\n",
            "Batch number 6/82\n",
            "Loss is 0.602619\n",
            "Epoch number 1/1\n",
            "Batch number 7/82\n",
            "Loss is 0.651250\n",
            "Epoch number 1/1\n",
            "Batch number 8/82\n",
            "Loss is 0.824269\n",
            "Epoch number 1/1\n",
            "Batch number 9/82\n",
            "Loss is 0.615098\n",
            "Epoch number 1/1\n",
            "Batch number 10/82\n",
            "Loss is 1.038185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b1127ddca79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/fft/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b1127ddca79b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/fft/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBxL9f7KEVbK"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "\r\n",
        "    correct = 0\r\n",
        "    preictal = 0\r\n",
        "    interictal = 0\r\n",
        "    totals = [0,0]\r\n",
        "    count = 0\r\n",
        "\r\n",
        "    for batch in batches:\r\n",
        "      count += 1\r\n",
        "      print(count)\r\n",
        "      labels = [int('preictal' in i) for i in batch]\r\n",
        "      x = tensor([np.load('/content/drive/MyDrive/fft/' + file)[:20000] for file in batch])\r\n",
        "      x = x.unsqueeze(1)\r\n",
        "      output = net(x)\r\n",
        "\r\n",
        "      for idx,i in enumerate(output):\r\n",
        "        totals[labels[idx]] += 1\r\n",
        "        if torch.argmax(i) == labels[idx]:\r\n",
        "          correct += 1\r\n",
        "          if labels[idx] == 0:\r\n",
        "            interictal += 1\r\n",
        "          else:\r\n",
        "            preictal += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DiMTJlpORlG",
        "outputId": "bac62626-046d-45a6-ff38-8af400818e70"
      },
      "source": [
        "accuracy = correct/len(filenames)\r\n",
        "print(accuracy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.765040650406504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ros37Ia9aaDS",
        "outputId": "bf0b992f-27eb-401a-e493-f853f8736ca6"
      },
      "source": [
        "acc_interictal = interictal/totals[0]\r\n",
        "acc_preictal = preictal/totals[1]\r\n",
        "print(acc_interictal,acc_preictal)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9357224118316269 0.33760683760683763\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}