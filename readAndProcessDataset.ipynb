{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "readAndProcessDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jd7T2s59uzG"
      },
      "source": [
        "\"\"\"\r\n",
        "Seizure Prediction from intracranial EEG\r\n",
        "DACO 2020/2021\r\n",
        "\r\n",
        "This notebook reads all the data from the training and test set and saves it after\r\n",
        "applying the fast fourier transform to the EEG data.\r\n",
        "\r\n",
        "@authors: Ana Maria Sousa, Mariana Xavier, Rui Santos\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTZUVduIMHPV",
        "outputId": "669a07a4-153d-4974-ddf0-ed8404731373"
      },
      "source": [
        "\"\"\"\r\n",
        "Training Set\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import os\r\n",
        "import scipy.io\r\n",
        "import numpy as np\r\n",
        "from scipy.fft import rfft\r\n",
        "\r\n",
        "# Run the training dir.\r\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/dataset/train'):\r\n",
        "       \r\n",
        "  # Read each of the files.\r\n",
        "  for filename in filenames:\r\n",
        "    print(filename)\r\n",
        "      \r\n",
        "    # Load the data from each file.\r\n",
        "    data = scipy.io.loadmat(os.path.join(dirname, filename))\r\n",
        "           \r\n",
        "    # Get the field corresponding to the segments.\r\n",
        "    train_data = data.get(list(data.keys())[3])\r\n",
        "            \r\n",
        "    # Get in a separate array the values of EEG for the different channels.\r\n",
        "    data = train_data['data'][0][0]\r\n",
        "    \r\n",
        "    # Considering all the different channels as representative of the EEG.\r\n",
        "    for channel in range (data.shape[0]):\r\n",
        "        # Saves the files after aplying the fft transform in the drive in \r\n",
        "        # format .npy\r\n",
        "        np.save('/content/drive/MyDrive/fft' + filename[:len(filename)-4] + '_channel_' + str(channel) + '.npy',abs(rfft(data[channel])))\r\n",
        "\r\n",
        "print(\"Training set read and fft successfully applied.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Patient_1_interictal_segment_0001.mat\n",
            "Patient_1_interictal_segment_0002.mat\n",
            "Patient_1_interictal_segment_0003.mat\n",
            "Patient_1_interictal_segment_0004.mat\n",
            "Patient_1_interictal_segment_0005.mat\n",
            "Patient_1_interictal_segment_0006.mat\n",
            "Patient_1_interictal_segment_0007.mat\n",
            "Patient_1_interictal_segment_0008.mat\n",
            "Patient_1_interictal_segment_0009.mat\n",
            "Patient_1_interictal_segment_0010.mat\n",
            "Patient_1_interictal_segment_0011.mat\n",
            "Patient_1_interictal_segment_0012.mat\n",
            "Patient_1_interictal_segment_0013.mat\n",
            "Patient_1_interictal_segment_0014.mat\n",
            "Patient_1_interictal_segment_0015.mat\n",
            "Patient_1_interictal_segment_0016.mat\n",
            "Patient_1_interictal_segment_0017.mat\n",
            "Patient_1_interictal_segment_0018.mat\n",
            "Patient_1_interictal_segment_0019.mat\n",
            "Patient_1_interictal_segment_0020.mat\n",
            "Patient_1_interictal_segment_0021.mat\n",
            "Patient_1_interictal_segment_0022.mat\n",
            "Patient_1_interictal_segment_0023.mat\n",
            "Patient_1_interictal_segment_0024.mat\n",
            "Patient_1_interictal_segment_0025.mat\n",
            "Patient_1_interictal_segment_0026.mat\n",
            "Patient_1_interictal_segment_0027.mat\n",
            "Patient_1_interictal_segment_0028.mat\n",
            "Patient_1_interictal_segment_0029.mat\n",
            "Patient_1_interictal_segment_0030.mat\n",
            "Patient_1_interictal_segment_0031.mat\n",
            "Patient_1_interictal_segment_0032.mat\n",
            "Patient_1_interictal_segment_0033.mat\n",
            "Patient_1_interictal_segment_0034.mat\n",
            "Patient_1_interictal_segment_0035.mat\n",
            "Patient_1_interictal_segment_0036.mat\n",
            "Patient_1_interictal_segment_0037.mat\n",
            "Patient_1_interictal_segment_0038.mat\n",
            "Patient_1_interictal_segment_0039.mat\n",
            "Patient_1_interictal_segment_0040.mat\n",
            "Patient_1_interictal_segment_0041.mat\n",
            "Patient_1_interictal_segment_0042.mat\n",
            "Patient_1_interictal_segment_0043.mat\n",
            "Patient_1_interictal_segment_0044.mat\n",
            "Patient_1_interictal_segment_0045.mat\n",
            "Patient_1_interictal_segment_0046.mat\n",
            "Patient_1_interictal_segment_0047.mat\n",
            "Patient_1_interictal_segment_0048.mat\n",
            "Patient_1_interictal_segment_0049.mat\n",
            "Patient_1_interictal_segment_0050.mat\n",
            "Patient_1_preictal_segment_0001.mat\n",
            "Patient_1_preictal_segment_0002.mat\n",
            "Patient_1_preictal_segment_0003.mat\n",
            "Patient_1_preictal_segment_0004.mat\n",
            "Patient_1_preictal_segment_0005.mat\n",
            "Patient_1_preictal_segment_0006.mat\n",
            "Patient_1_preictal_segment_0007.mat\n",
            "Patient_1_preictal_segment_0008.mat\n",
            "Patient_1_preictal_segment_0009.mat\n",
            "Patient_1_preictal_segment_0010.mat\n",
            "Patient_1_preictal_segment_0011.mat\n",
            "Patient_1_preictal_segment_0012.mat\n",
            "Patient_1_preictal_segment_0013.mat\n",
            "Patient_1_preictal_segment_0014.mat\n",
            "Patient_1_preictal_segment_0015.mat\n",
            "Patient_1_preictal_segment_0016.mat\n",
            "Patient_1_preictal_segment_0017.mat\n",
            "Patient_1_preictal_segment_0018.mat\n",
            "Patient_2_interictal_segment_0001.mat\n",
            "Patient_2_interictal_segment_0002.mat\n",
            "Patient_2_interictal_segment_0003.mat\n",
            "Patient_2_interictal_segment_0004.mat\n",
            "Patient_2_interictal_segment_0005.mat\n",
            "Patient_2_interictal_segment_0006.mat\n",
            "Patient_2_interictal_segment_0007.mat\n",
            "Patient_2_interictal_segment_0008.mat\n",
            "Patient_2_interictal_segment_0009.mat\n",
            "Patient_2_interictal_segment_0010.mat\n",
            "Patient_2_interictal_segment_0011.mat\n",
            "Patient_2_interictal_segment_0012.mat\n",
            "Patient_2_interictal_segment_0013.mat\n",
            "Patient_2_interictal_segment_0014.mat\n",
            "Patient_2_interictal_segment_0015.mat\n",
            "Patient_2_interictal_segment_0016.mat\n",
            "Patient_2_interictal_segment_0017.mat\n",
            "Patient_2_interictal_segment_0018.mat\n",
            "Patient_2_interictal_segment_0019.mat\n",
            "Patient_2_interictal_segment_0020.mat\n",
            "Patient_2_interictal_segment_0021.mat\n",
            "Patient_2_interictal_segment_0022.mat\n",
            "Patient_2_interictal_segment_0023.mat\n",
            "Patient_2_interictal_segment_0024.mat\n",
            "Patient_2_interictal_segment_0025.mat\n",
            "Patient_2_interictal_segment_0026.mat\n",
            "Patient_2_interictal_segment_0027.mat\n",
            "Patient_2_interictal_segment_0028.mat\n",
            "Patient_2_interictal_segment_0029.mat\n",
            "Patient_2_interictal_segment_0030.mat\n",
            "Patient_2_interictal_segment_0031.mat\n",
            "Patient_2_interictal_segment_0032.mat\n",
            "Patient_2_interictal_segment_0033.mat\n",
            "Patient_2_interictal_segment_0034.mat\n",
            "Patient_2_interictal_segment_0035.mat\n",
            "Patient_2_interictal_segment_0036.mat\n",
            "Patient_2_interictal_segment_0037.mat\n",
            "Patient_2_interictal_segment_0038.mat\n",
            "Patient_2_interictal_segment_0039.mat\n",
            "Patient_2_interictal_segment_0040.mat\n",
            "Patient_2_interictal_segment_0041.mat\n",
            "Patient_2_interictal_segment_0042.mat\n",
            "Patient_2_preictal_segment_0001.mat\n",
            "Patient_2_preictal_segment_0002.mat\n",
            "Patient_2_preictal_segment_0003.mat\n",
            "Patient_2_preictal_segment_0004.mat\n",
            "Patient_2_preictal_segment_0005.mat\n",
            "Patient_2_preictal_segment_0006.mat\n",
            "Patient_2_preictal_segment_0007.mat\n",
            "Patient_2_preictal_segment_0008.mat\n",
            "Patient_2_preictal_segment_0009.mat\n",
            "Patient_2_preictal_segment_0010.mat\n",
            "Patient_2_preictal_segment_0011.mat\n",
            "Patient_2_preictal_segment_0012.mat\n",
            "Patient_2_preictal_segment_0013.mat\n",
            "Patient_2_preictal_segment_0014.mat\n",
            "Patient_2_preictal_segment_0015.mat\n",
            "Patient_2_preictal_segment_0016.mat\n",
            "Patient_2_preictal_segment_0017.mat\n",
            "Patient_2_preictal_segment_0018.mat\n",
            "Training set read and fft successfully applied.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLAj9WiN94Fu"
      },
      "source": [
        "\"\"\"\r\n",
        "Test Set\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import os\r\n",
        "import scipy.io\r\n",
        "import numpy as np\r\n",
        "from scipy.fft import rfft\r\n",
        "\r\n",
        "# Run the training dir.\r\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/dataset/test'):\r\n",
        "       \r\n",
        "  # Read each of the files.\r\n",
        "  for filename in filenames:\r\n",
        "    print(filename)\r\n",
        "      \r\n",
        "    # Load the data from each file.\r\n",
        "    data = scipy.io.loadmat(os.path.join(dirname, filename))\r\n",
        "           \r\n",
        "    # Get the field corresponding to the segments.\r\n",
        "    train_data = data.get(list(data.keys())[3])\r\n",
        "            \r\n",
        "    # Get in a separate array the values of EEG for the different channels.\r\n",
        "    data = train_data['data'][0][0]\r\n",
        "    \r\n",
        "    # Considering all the different channels as representative of the EEG.\r\n",
        "    for channel in range (data.shape[0]):\r\n",
        "        # Saves the files after aplying the fft transform in the drive in \r\n",
        "        # format .npy\r\n",
        "        np.save('/content/drive/MyDrive/fft' + filename[:len(filename)-4] + '_channel_' + str(channel) + '.npy',abs(rfft(data[channel])))\r\n",
        "\r\n",
        "print(\"Test set read and fft successfully applied.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}